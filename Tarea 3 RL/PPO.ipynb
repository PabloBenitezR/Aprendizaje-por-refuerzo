{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== pipsas =======\n",
    "\n",
    "# implementaciÃ³n\n",
    "import gymnasium as gym \n",
    "import ale_py         \n",
    "import torch                    \n",
    "import torch.nn as nn           \n",
    "import torch.optim as optim     \n",
    "import numpy as np             \n",
    "from torch.distributions import Categorical\n",
    "from torchvision import transforms \n",
    "\n",
    "# graficas\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "# guardar y cargar \n",
    "import os    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== max rendimiento ======\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== normalizar el espacio ======\n",
    "\n",
    "normaliza = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def normaliza_frame(frame):\n",
    "    return normaliza(frame).squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== actor crÃ­tico ======\n",
    "\n",
    "class ActorC(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.politica_up = nn.Linear(512, action_dim)\n",
    "        self.valor_up = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return torch.softmax(self.politica_up(x), dim=-1), self.valor_up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== retorno ======\n",
    "\n",
    "def retorno(rewards, dones, valores, gamma=0.99):\n",
    "    retornos, G = [], 0\n",
    "    for r, done, v in zip(reversed(rewards), reversed(dones), reversed(valores)):\n",
    "\n",
    "        G = r + gamma * G * (1 - done) # funciÃ³n de retorno\n",
    "        retornos.insert(0, G)\n",
    "\n",
    "    retornos = torch.tensor(retornos).float().to(device)\n",
    "    ventajas = retornos - valores\n",
    "    return retornos, ventajas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== entrenamiento ======\n",
    "\n",
    "env = gym.make('ALE/Breakout-v5', render_mode=None, frameskip=1)\n",
    "num_actions = env.action_space.n\n",
    "print(f\"Acciones posibles: {num_actions}\")\n",
    "\n",
    "model = ActorC(num_actions).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2.5e-4)\n",
    "\n",
    "# ðŸ”¥ Punto de guardado\n",
    "check = 'ppo_breakout.pth'\n",
    "if os.path.exists(check):\n",
    "    model.load_state_dict(torch.load(check, map_location=device))\n",
    "    print(\"âœ… Checkpoint cargado.\")\n",
    "else:\n",
    "    print(\"ðŸš€ Empezamos entrenamiento desde cero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== datos del entrenamiento ======\n",
    "\n",
    "episodios = 200\n",
    "ppo_episodios = 4\n",
    "batch_size = 2048\n",
    "gamma = 0.99\n",
    "clip_eps = 0.1\n",
    "entropy_coef = 0.01\n",
    "\n",
    "rewards_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== implementaciÃ³n del entrenamiento ======\n",
    "\n",
    "for ep in range(episodios):\n",
    "    states, actions, rewards, dones, log_probs, valores = [], [], [], [], [], []\n",
    "    obs, _ = env.reset()\n",
    "    frame = normaliza_frame(obs)\n",
    "    state_stack = np.stack([frame] * 4, axis=0)\n",
    "    total_reward = 0\n",
    "\n",
    "    while len(states) < batch_size:\n",
    "        state_tensor = torch.FloatTensor(state_stack).unsqueeze(0).to(device)\n",
    "        probs, value = model(state_tensor)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_frame = normaliza_frame(next_obs)\n",
    "        next_state_stack = np.append(state_stack[1:], [next_frame], axis=0)\n",
    "\n",
    "        states.append(state_stack)\n",
    "        actions.append(action.item())\n",
    "        rewards.append(reward)\n",
    "        dones.append(float(done))\n",
    "        log_probs.append(dist.log_prob(action).item())\n",
    "        valores.append(value.item())\n",
    "        state_stack = next_state_stack\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "            frame = normaliza_frame(obs)\n",
    "            state_stack = np.stack([frame] * 4, axis=0)\n",
    "\n",
    "    rewards_h.append(total_reward)\n",
    "    valores_tensor = torch.tensor(valores).float().to(device)\n",
    "    retornos, ventajas = retorno(rewards, dones, valores_tensor, gamma)\n",
    "    ventajas = (ventajas - ventajas.mean()) / (ventajas.std() + 1e-8)\n",
    "\n",
    "    states_tensor = torch.FloatTensor(np.stack(states)).to(device)\n",
    "    actions_tensor = torch.LongTensor(actions).to(device)\n",
    "    old_log_probs_tensor = torch.FloatTensor(log_probs).to(device)\n",
    "\n",
    "    for _ in range(ppo_episodios):\n",
    "        probs, state_valores = model(states_tensor)\n",
    "        dist = Categorical(probs)\n",
    "        new_log_probs = dist.log_prob(actions_tensor)\n",
    "        ratios = torch.exp(new_log_probs - old_log_probs_tensor)\n",
    "\n",
    "        surr1 = ratios * ventajas\n",
    "        surr2 = torch.clamp(ratios, 1 - clip_eps, 1 + clip_eps) * ventajas\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (state_valores.squeeze() - retornos).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "        loss = actor_loss + 0.5 * critic_loss - entropy_coef * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # guardar cada 25 episodios\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        torch.save(model.state_dict(), check)\n",
    "        print(f\"ðŸ’¾ Checkpoint guardado en episodio {ep + 1}\")\n",
    "\n",
    "    print(f\"Episodio {ep + 1}, Recompensa total: {total_reward}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== guardar el Ãºltimo modelo ======\n",
    "\n",
    "torch.save(model.state_dict(), check)\n",
    "print(\"Entrenamiento finalizado y modelo guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== grafica de las recompensas ======\n",
    "\n",
    "plt.plot(rewards_h)\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.title('PPO Breakout MinAtar - Recompensa por episodio')\n",
    "plt.show()\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
