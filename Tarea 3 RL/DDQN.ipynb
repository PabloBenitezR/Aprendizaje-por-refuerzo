{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== pipsas =======\n",
    "\n",
    "# implementaci√≥n\n",
    "import gymnasium as gym \n",
    "import ale_py         \n",
    "import torch                    \n",
    "import torch.nn as nn           \n",
    "import torch.optim as optim     \n",
    "import numpy as np             \n",
    "import random                   \n",
    "from collections import deque  \n",
    "from torchvision import transforms \n",
    "\n",
    "# graficas\n",
    "import matplotlib.pyplot as plt     \n",
    "\n",
    "# guardar y cargar \n",
    "import os                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== max rendimiento ======\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== normalizar el espacio ======\n",
    "\n",
    "normaliza = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((84, 84)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== red neuronal ======\n",
    "\n",
    "class QN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(QN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== agente ======\n",
    "\n",
    "class QNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000) \n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99    # descuento \n",
    "        self.tau = 1e-3      # soft update de la red target\n",
    "        self.lr = 1e-4       # tasa de aprendizaje\n",
    "\n",
    "        # red neuronal\n",
    "        self.politica_net = QN(action_size).to(device)\n",
    "        # red target\n",
    "        self.target_net = QN(action_size).to(device)\n",
    "\n",
    "    \n",
    "        self.optimizer = optim.Adam(self.politica_net.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.act_target_net()\n",
    "\n",
    "    # guardar las transiciones\n",
    "    def memoria(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # pol√≠tica epsilon-greedy\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.politica_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(np.stack(states)).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.stack(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "        current_q = self.politica_net(states).gather(1, actions)\n",
    "        next_actions = torch.argmax(self.politica_net(next_states), dim=1, keepdim=True)\n",
    "        next_q = self.target_net(next_states).gather(1, next_actions)\n",
    "\n",
    "        target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        loss = self.criterion(current_q, target_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # actualizar la red target\n",
    "    def act_target_net(self):\n",
    "        self.target_net.load_state_dict(self.politica_net.state_dict())\n",
    "\n",
    "    # guargar el modelo\n",
    "    def save(self, filename):\n",
    "        torch.save(self.politica_net.state_dict(), filename)\n",
    "\n",
    "    # cargar el modelo (de haber uno claro)\n",
    "    def load(self, filename):\n",
    "        self.politica_net.load_state_dict(torch.load(filename, map_location=device))\n",
    "        self.politica_net.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== normaliza los frames ======\n",
    "\n",
    "def normaliza_frame(frame):\n",
    "    frame = normaliza(frame).squeeze(0)\n",
    "    return frame.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acciones posibles: 4\n",
      "üöÄ No hay checkpoint. Empezando desde cero...\n"
     ]
    }
   ],
   "source": [
    "# ====== entrenamiento ======\n",
    "\n",
    "env = gym.make('ALE/Breakout-v5', render_mode=None, frameskip=1)\n",
    "num_actions = env.action_space.n\n",
    "print(f\"Acciones posibles: {num_actions}\")\n",
    "\n",
    "agent = QNAgent(num_actions)\n",
    "\n",
    "# üî• Punto de guardado\n",
    "checkpoint_path = 'ddqn_breakout.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"‚úÖ Checkpoint encontrado. Cargando...\")\n",
    "    agent.load(checkpoint_path)\n",
    "else:\n",
    "    print(\"üöÄ No hay checkpoint. Empezando desde cero...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== datos del entrenamiento ======\n",
    "epsodios = 200\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 10\n",
    "\n",
    "rewards_h = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1, Recompensa total: 0.0, Epsilon: 0.995\n",
      "Episodio 2, Recompensa total: 0.0, Epsilon: 0.990\n",
      "Episodio 3, Recompensa total: 0.0, Epsilon: 0.985\n",
      "Episodio 4, Recompensa total: 3.0, Epsilon: 0.980\n",
      "Episodio 5, Recompensa total: 0.0, Epsilon: 0.975\n",
      "Episodio 6, Recompensa total: 1.0, Epsilon: 0.970\n",
      "Episodio 7, Recompensa total: 2.0, Epsilon: 0.966\n",
      "Episodio 8, Recompensa total: 5.0, Epsilon: 0.961\n",
      "Episodio 9, Recompensa total: 1.0, Epsilon: 0.956\n",
      "Episodio 10, Recompensa total: 2.0, Epsilon: 0.951\n",
      "Episodio 11, Recompensa total: 1.0, Epsilon: 0.946\n",
      "Episodio 12, Recompensa total: 3.0, Epsilon: 0.942\n",
      "Episodio 13, Recompensa total: 0.0, Epsilon: 0.937\n",
      "Episodio 14, Recompensa total: 3.0, Epsilon: 0.932\n",
      "Episodio 15, Recompensa total: 0.0, Epsilon: 0.928\n",
      "Episodio 16, Recompensa total: 1.0, Epsilon: 0.923\n",
      "Episodio 17, Recompensa total: 2.0, Epsilon: 0.918\n",
      "Episodio 18, Recompensa total: 4.0, Epsilon: 0.914\n",
      "Episodio 19, Recompensa total: 2.0, Epsilon: 0.909\n",
      "Episodio 20, Recompensa total: 1.0, Epsilon: 0.905\n",
      "Episodio 21, Recompensa total: 3.0, Epsilon: 0.900\n",
      "Episodio 22, Recompensa total: 13.0, Epsilon: 0.896\n",
      "Episodio 23, Recompensa total: 2.0, Epsilon: 0.891\n",
      "Episodio 24, Recompensa total: 1.0, Epsilon: 0.887\n",
      "üíæ Checkpoint guardado en episodio 25\n",
      "Episodio 25, Recompensa total: 0.0, Epsilon: 0.882\n",
      "Episodio 26, Recompensa total: 5.0, Epsilon: 0.878\n",
      "Episodio 27, Recompensa total: 2.0, Epsilon: 0.873\n",
      "Episodio 28, Recompensa total: 0.0, Epsilon: 0.869\n",
      "Episodio 29, Recompensa total: 0.0, Epsilon: 0.865\n",
      "Episodio 30, Recompensa total: 3.0, Epsilon: 0.860\n",
      "Episodio 31, Recompensa total: 2.0, Epsilon: 0.856\n",
      "Episodio 32, Recompensa total: 2.0, Epsilon: 0.852\n",
      "Episodio 33, Recompensa total: 3.0, Epsilon: 0.848\n",
      "Episodio 34, Recompensa total: 2.0, Epsilon: 0.843\n",
      "Episodio 35, Recompensa total: 1.0, Epsilon: 0.839\n",
      "Episodio 36, Recompensa total: 1.0, Epsilon: 0.835\n",
      "Episodio 37, Recompensa total: 1.0, Epsilon: 0.831\n",
      "Episodio 38, Recompensa total: 2.0, Epsilon: 0.827\n",
      "Episodio 39, Recompensa total: 0.0, Epsilon: 0.822\n",
      "Episodio 40, Recompensa total: 1.0, Epsilon: 0.818\n",
      "Episodio 41, Recompensa total: 1.0, Epsilon: 0.814\n",
      "Episodio 42, Recompensa total: 0.0, Epsilon: 0.810\n",
      "Episodio 43, Recompensa total: 2.0, Epsilon: 0.806\n",
      "Episodio 44, Recompensa total: 1.0, Epsilon: 0.802\n",
      "Episodio 45, Recompensa total: 5.0, Epsilon: 0.798\n",
      "Episodio 46, Recompensa total: 1.0, Epsilon: 0.794\n",
      "Episodio 47, Recompensa total: 1.0, Epsilon: 0.790\n",
      "Episodio 48, Recompensa total: 0.0, Epsilon: 0.786\n",
      "Episodio 49, Recompensa total: 3.0, Epsilon: 0.782\n",
      "üíæ Checkpoint guardado en episodio 50\n",
      "Episodio 50, Recompensa total: 1.0, Epsilon: 0.778\n",
      "Episodio 51, Recompensa total: 4.0, Epsilon: 0.774\n",
      "Episodio 52, Recompensa total: 3.0, Epsilon: 0.771\n",
      "Episodio 53, Recompensa total: 2.0, Epsilon: 0.767\n",
      "Episodio 54, Recompensa total: 0.0, Epsilon: 0.763\n",
      "Episodio 55, Recompensa total: 2.0, Epsilon: 0.759\n",
      "Episodio 56, Recompensa total: 1.0, Epsilon: 0.755\n",
      "Episodio 57, Recompensa total: 1.0, Epsilon: 0.751\n",
      "Episodio 58, Recompensa total: 2.0, Epsilon: 0.748\n",
      "Episodio 59, Recompensa total: 7.0, Epsilon: 0.744\n",
      "Episodio 60, Recompensa total: 1.0, Epsilon: 0.740\n",
      "Episodio 61, Recompensa total: 2.0, Epsilon: 0.737\n",
      "Episodio 62, Recompensa total: 4.0, Epsilon: 0.733\n",
      "Episodio 63, Recompensa total: 2.0, Epsilon: 0.729\n",
      "Episodio 64, Recompensa total: 1.0, Epsilon: 0.726\n",
      "Episodio 65, Recompensa total: 0.0, Epsilon: 0.722\n",
      "Episodio 66, Recompensa total: 3.0, Epsilon: 0.718\n",
      "Episodio 67, Recompensa total: 3.0, Epsilon: 0.715\n",
      "Episodio 68, Recompensa total: 3.0, Epsilon: 0.711\n",
      "Episodio 69, Recompensa total: 1.0, Epsilon: 0.708\n",
      "Episodio 70, Recompensa total: 2.0, Epsilon: 0.704\n",
      "Episodio 71, Recompensa total: 3.0, Epsilon: 0.701\n",
      "Episodio 72, Recompensa total: 0.0, Epsilon: 0.697\n",
      "Episodio 73, Recompensa total: 0.0, Epsilon: 0.694\n",
      "Episodio 74, Recompensa total: 1.0, Epsilon: 0.690\n",
      "üíæ Checkpoint guardado en episodio 75\n",
      "Episodio 75, Recompensa total: 0.0, Epsilon: 0.687\n",
      "Episodio 76, Recompensa total: 0.0, Epsilon: 0.683\n",
      "Episodio 77, Recompensa total: 2.0, Epsilon: 0.680\n",
      "Episodio 78, Recompensa total: 0.0, Epsilon: 0.676\n",
      "Episodio 79, Recompensa total: 0.0, Epsilon: 0.673\n",
      "Episodio 80, Recompensa total: 0.0, Epsilon: 0.670\n",
      "Episodio 81, Recompensa total: 1.0, Epsilon: 0.666\n",
      "Episodio 82, Recompensa total: 1.0, Epsilon: 0.663\n",
      "Episodio 83, Recompensa total: 1.0, Epsilon: 0.660\n",
      "Episodio 84, Recompensa total: 2.0, Epsilon: 0.656\n",
      "Episodio 85, Recompensa total: 2.0, Epsilon: 0.653\n",
      "Episodio 86, Recompensa total: 2.0, Epsilon: 0.650\n",
      "Episodio 87, Recompensa total: 3.0, Epsilon: 0.647\n",
      "Episodio 88, Recompensa total: 3.0, Epsilon: 0.643\n",
      "Episodio 89, Recompensa total: 4.0, Epsilon: 0.640\n",
      "Episodio 90, Recompensa total: 4.0, Epsilon: 0.637\n",
      "Episodio 91, Recompensa total: 3.0, Epsilon: 0.634\n",
      "Episodio 92, Recompensa total: 2.0, Epsilon: 0.631\n",
      "Episodio 93, Recompensa total: 3.0, Epsilon: 0.627\n",
      "Episodio 94, Recompensa total: 1.0, Epsilon: 0.624\n",
      "Episodio 95, Recompensa total: 0.0, Epsilon: 0.621\n",
      "Episodio 96, Recompensa total: 1.0, Epsilon: 0.618\n",
      "Episodio 97, Recompensa total: 1.0, Epsilon: 0.615\n",
      "Episodio 98, Recompensa total: 1.0, Epsilon: 0.612\n",
      "Episodio 99, Recompensa total: 2.0, Epsilon: 0.609\n",
      "üíæ Checkpoint guardado en episodio 100\n",
      "Episodio 100, Recompensa total: 2.0, Epsilon: 0.606\n",
      "Episodio 101, Recompensa total: 2.0, Epsilon: 0.603\n",
      "Episodio 102, Recompensa total: 3.0, Epsilon: 0.600\n",
      "Episodio 103, Recompensa total: 0.0, Epsilon: 0.597\n",
      "Episodio 104, Recompensa total: 2.0, Epsilon: 0.594\n",
      "Episodio 105, Recompensa total: 2.0, Epsilon: 0.591\n",
      "Episodio 106, Recompensa total: 1.0, Epsilon: 0.588\n",
      "Episodio 107, Recompensa total: 2.0, Epsilon: 0.585\n",
      "Episodio 108, Recompensa total: 0.0, Epsilon: 0.582\n",
      "Episodio 109, Recompensa total: 0.0, Epsilon: 0.579\n",
      "Episodio 110, Recompensa total: 4.0, Epsilon: 0.576\n",
      "Episodio 111, Recompensa total: 0.0, Epsilon: 0.573\n",
      "Episodio 112, Recompensa total: 4.0, Epsilon: 0.570\n",
      "Episodio 113, Recompensa total: 0.0, Epsilon: 0.568\n",
      "Episodio 114, Recompensa total: 3.0, Epsilon: 0.565\n",
      "Episodio 115, Recompensa total: 1.0, Epsilon: 0.562\n",
      "Episodio 116, Recompensa total: 2.0, Epsilon: 0.559\n",
      "Episodio 117, Recompensa total: 0.0, Epsilon: 0.556\n",
      "Episodio 118, Recompensa total: 1.0, Epsilon: 0.554\n",
      "Episodio 119, Recompensa total: 1.0, Epsilon: 0.551\n",
      "Episodio 120, Recompensa total: 1.0, Epsilon: 0.548\n",
      "Episodio 121, Recompensa total: 1.0, Epsilon: 0.545\n",
      "Episodio 122, Recompensa total: 0.0, Epsilon: 0.543\n",
      "Episodio 123, Recompensa total: 1.0, Epsilon: 0.540\n",
      "Episodio 124, Recompensa total: 0.0, Epsilon: 0.537\n",
      "üíæ Checkpoint guardado en episodio 125\n",
      "Episodio 125, Recompensa total: 2.0, Epsilon: 0.534\n",
      "Episodio 126, Recompensa total: 0.0, Epsilon: 0.532\n",
      "Episodio 127, Recompensa total: 1.0, Epsilon: 0.529\n",
      "Episodio 128, Recompensa total: 2.0, Epsilon: 0.526\n",
      "Episodio 129, Recompensa total: 2.0, Epsilon: 0.524\n",
      "Episodio 130, Recompensa total: 3.0, Epsilon: 0.521\n",
      "Episodio 131, Recompensa total: 0.0, Epsilon: 0.519\n",
      "Episodio 132, Recompensa total: 2.0, Epsilon: 0.516\n",
      "Episodio 133, Recompensa total: 2.0, Epsilon: 0.513\n",
      "Episodio 134, Recompensa total: 3.0, Epsilon: 0.511\n",
      "Episodio 135, Recompensa total: 1.0, Epsilon: 0.508\n",
      "Episodio 136, Recompensa total: 2.0, Epsilon: 0.506\n",
      "Episodio 137, Recompensa total: 0.0, Epsilon: 0.503\n",
      "Episodio 138, Recompensa total: 1.0, Epsilon: 0.501\n",
      "Episodio 139, Recompensa total: 3.0, Epsilon: 0.498\n",
      "Episodio 140, Recompensa total: 1.0, Epsilon: 0.496\n",
      "Episodio 141, Recompensa total: 4.0, Epsilon: 0.493\n",
      "Episodio 142, Recompensa total: 4.0, Epsilon: 0.491\n",
      "Episodio 143, Recompensa total: 0.0, Epsilon: 0.488\n",
      "Episodio 144, Recompensa total: 0.0, Epsilon: 0.486\n",
      "Episodio 145, Recompensa total: 2.0, Epsilon: 0.483\n",
      "Episodio 146, Recompensa total: 2.0, Epsilon: 0.481\n",
      "Episodio 147, Recompensa total: 4.0, Epsilon: 0.479\n",
      "Episodio 148, Recompensa total: 2.0, Epsilon: 0.476\n",
      "Episodio 149, Recompensa total: 4.0, Epsilon: 0.474\n",
      "üíæ Checkpoint guardado en episodio 150\n",
      "Episodio 150, Recompensa total: 2.0, Epsilon: 0.471\n",
      "Episodio 151, Recompensa total: 0.0, Epsilon: 0.469\n",
      "Episodio 152, Recompensa total: 3.0, Epsilon: 0.467\n",
      "Episodio 153, Recompensa total: 2.0, Epsilon: 0.464\n",
      "Episodio 154, Recompensa total: 2.0, Epsilon: 0.462\n",
      "Episodio 155, Recompensa total: 2.0, Epsilon: 0.460\n",
      "Episodio 156, Recompensa total: 3.0, Epsilon: 0.458\n",
      "Episodio 157, Recompensa total: 0.0, Epsilon: 0.455\n",
      "Episodio 158, Recompensa total: 2.0, Epsilon: 0.453\n",
      "Episodio 159, Recompensa total: 2.0, Epsilon: 0.451\n",
      "Episodio 160, Recompensa total: 0.0, Epsilon: 0.448\n",
      "Episodio 161, Recompensa total: 1.0, Epsilon: 0.446\n",
      "Episodio 162, Recompensa total: 2.0, Epsilon: 0.444\n",
      "Episodio 163, Recompensa total: 0.0, Epsilon: 0.442\n",
      "Episodio 164, Recompensa total: 2.0, Epsilon: 0.440\n",
      "Episodio 165, Recompensa total: 0.0, Epsilon: 0.437\n",
      "Episodio 166, Recompensa total: 0.0, Epsilon: 0.435\n",
      "Episodio 167, Recompensa total: 3.0, Epsilon: 0.433\n",
      "Episodio 168, Recompensa total: 1.0, Epsilon: 0.431\n",
      "Episodio 169, Recompensa total: 1.0, Epsilon: 0.429\n",
      "Episodio 170, Recompensa total: 1.0, Epsilon: 0.427\n",
      "Episodio 171, Recompensa total: 1.0, Epsilon: 0.424\n",
      "Episodio 172, Recompensa total: 4.0, Epsilon: 0.422\n",
      "Episodio 173, Recompensa total: 3.0, Epsilon: 0.420\n",
      "Episodio 174, Recompensa total: 3.0, Epsilon: 0.418\n",
      "üíæ Checkpoint guardado en episodio 175\n",
      "Episodio 175, Recompensa total: 3.0, Epsilon: 0.416\n"
     ]
    }
   ],
   "source": [
    "# ====== implementaci√≥n del entrenamiento ======\n",
    "\n",
    "for ep in range(epsodios):\n",
    "    obs, _ = env.reset()\n",
    "    frame = normaliza_frame(obs)\n",
    "    state_stack = np.stack([frame] * 4, axis=0)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state_stack, epsilon)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_frame = normaliza_frame(next_obs)\n",
    "        next_state_stack = np.append(state_stack[1:], [next_frame], axis=0)\n",
    "\n",
    "        agent.memoria(state_stack, action, reward, next_state_stack, done)\n",
    "        state_stack = next_state_stack\n",
    "        total_reward += reward\n",
    "\n",
    "        agent.replay()\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    rewards_h.append(total_reward)\n",
    "\n",
    "    if ep % target_update_freq == 0:\n",
    "        agent.act_target_net()\n",
    "\n",
    "    # guardar cada 25 episodios\n",
    "    if (ep + 1) % 25 == 0:\n",
    "        agent.save(checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint guardado en episodio {ep + 1}\")\n",
    "\n",
    "    print(f\"Episodio {ep + 1}, Recompensa total: {total_reward}, Epsilon: {epsilon:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== guardar el √∫ltimo modelo ======\n",
    "\n",
    "agent.save(checkpoint_path)\n",
    "print(\"Entrenamiento terminado. Modelo guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== grafica de las recompensas ======\n",
    "\n",
    "plt.plot(rewards_h)\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Recompensa total')\n",
    "plt.title('DDQN Breakout MinAtar - Recompensa por episodio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
