{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paquetes\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mo_gymnasium as mo_gym\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcc99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parámetros\n",
    "env = mo_gym.make(\"mo-lunar-lander-v3\")\n",
    "num_act = env.action_space.n\n",
    "alpha = 0.3\n",
    "gamma = 0.999\n",
    "eps = 25000  # número de episodios\n",
    "\n",
    "# exploración\n",
    "epsilon_i = 1.0  # inicial\n",
    "epsilon_f = 0.01  # final\n",
    "epsilon_d = 0.9999  # decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90471745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea Q_set que guarda para cada estado \n",
    "_ = env.reset()\n",
    "_, r_vec, _, _, _ = env.step(env.action_space.sample())\n",
    "num_obj = len(r_vec)\n",
    "Q = defaultdict(lambda: np.zeros((num_act, num_obj)))  \n",
    "\n",
    "# diccs\n",
    "front = []                   # frente de Pareto\n",
    "r_hist = []                  # Recompensa por episodio\n",
    "pareto_t_hist = []           # tamaño del frente de Pareto\n",
    "t_traj = {}     # dicc de trayectorias que alcanzan el frente de Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83361f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discret(estado):\n",
    "    return tuple(np.round(estado, decimals=1))\n",
    "\n",
    "def selc_accion(estado, epsilon=0.1):\n",
    "    q_valores = Q[estado]\n",
    "    acc_nodom = [] # acciones no dominadas\n",
    "    for a in range(num_act):\n",
    "        dom = False\n",
    "        for b in range(num_act):\n",
    "            if a != b:\n",
    "                if np.all(q_valores[b] >= q_valores[a]) and np.any(q_valores[b] > q_valores[a]):\n",
    "                    dom = True\n",
    "                    break\n",
    "        if not dom:\n",
    "            acc_nodom.append(a)\n",
    "    if np.random.rand() < epsilon or len(acc_nodom) == 0:\n",
    "        return np.random.randint(num_act)\n",
    "    else:\n",
    "        return np.random.choice(acc_nodom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf72b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actualizar frente de Pareto\n",
    "def act(frontera, nuevo_vec):\n",
    "    no_dominados = []\n",
    "    for vec in frontera:\n",
    "        if np.all(vec >= nuevo_vec) and np.any(vec > nuevo_vec):\n",
    "            return frontera  \n",
    "        elif not (np.all(nuevo_vec >= vec) and np.any(nuevo_vec > vec)):\n",
    "            no_dominados.append(vec)\n",
    "    no_dominados.append(nuevo_vec)\n",
    "    return no_dominados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ep in range(eps):\n",
    "    epsilon_a = max(epsilon_f, epsilon_i * (epsilon_d ** ep))\n",
    "    \n",
    "    estado, _ = env.reset()\n",
    "    estado = discret(estado)\n",
    "    terminado = False\n",
    "    r_acc = np.zeros(num_obj)  \n",
    "    trajectory = []  \n",
    "    \n",
    "    while not terminado:\n",
    "        accion = selc_accion(estado, epsilon=epsilon_a)\n",
    "        nv_estado, r_vec, fin, tronco, _ = env.step(accion)\n",
    "        nv_estado = discret(nv_estado)\n",
    "        terminado = fin or tronco\n",
    "        \n",
    "        mejor_sig = np.max(Q[nv_estado], axis=0)\n",
    "        Q[estado][accion] = (1 - alpha) * Q[estado][accion] + alpha * (np.array(r_vec) + gamma * mejor_sig)\n",
    "        \n",
    "        trajectory.append((estado, accion, np.array(r_vec), nv_estado))\n",
    "        \n",
    "        estado = nv_estado\n",
    "        r_acc += np.array(r_vec)\n",
    "    \n",
    "    r_hist.append(r_acc)\n",
    "    front = act(front, r_acc)\n",
    "    pareto_t_hist.append(len(front))\n",
    "    \n",
    "    if np.isclose(r_acc[0], 100, atol=1e-3) and r_acc[0] > 0:\n",
    "        t_traj[ep] = trajectory\n",
    "        \n",
    "    if ep % 100 == 0:\n",
    "        print(f\"Episodio {ep} (ε = {epsilon_a:.3f}), recompensa: {r_acc}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
