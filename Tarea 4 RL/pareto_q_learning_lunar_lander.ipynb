{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521e9f94",
   "metadata": {},
   "source": [
    "# Pareto Q-Learning en `mo-lunar-lander-v3`\n",
    "\n",
    "Este notebook implementa el algoritmo de Pareto Q-Learning sobre el entorno multiobjetivo `mo-lunar-lander-v3` usando Python y `mo-gymnasium`.\n",
    "\n",
    "**Abreviaturas utilizadas**:\n",
    "- `acc`: acumulada\n",
    "- `est`: estados\n",
    "- `eps`: episodios\n",
    "- `obj`: objetivos\n",
    "- `act`: acciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mo_gymnasium as mo_gym\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entorno = mo_gym.make(\"mo-lunar-lander-v3\")\n",
    "num_act = entorno.action_space.n\n",
    "num_est = 1000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "eps = 500\n",
    "num_obj = len(entorno.reward_range)\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros((num_act, num_obj)))\n",
    "frontera_pareto = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discretizar(estado):\n",
    "    return tuple(np.round(estado, decimals=1))\n",
    "\n",
    "def seleccionar_accion(estado):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(num_act)\n",
    "    else:\n",
    "        valores = Q[estado]\n",
    "        escalarizados = valores @ np.ones(num_obj)\n",
    "        return np.argmax(escalarizados)\n",
    "\n",
    "def actualizar_pareto(frontera, nuevo_vec):\n",
    "    no_dominados = []\n",
    "    for vec in frontera:\n",
    "        if np.all(vec >= nuevo_vec) and np.any(vec > nuevo_vec):\n",
    "            return frontera\n",
    "        elif not (np.all(nuevo_vec >= vec) and np.any(nuevo_vec > vec)):\n",
    "            no_dominados.append(vec)\n",
    "    no_dominados.append(nuevo_vec)\n",
    "    return no_dominados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recompensas_hist = []\n",
    "\n",
    "for ep in range(eps):\n",
    "    estado, _ = entorno.reset()\n",
    "    estado = discretizar(estado)\n",
    "    terminado = False\n",
    "    recompensa_acc = np.zeros(num_obj)\n",
    "\n",
    "    while not terminado:\n",
    "        accion = seleccionar_accion(estado)\n",
    "        nuevo_estado, recompensa_vec, fin, truncado, _ = entorno.step(accion)\n",
    "        nuevo_estado = discretizar(nuevo_estado)\n",
    "        terminado = fin or truncado\n",
    "\n",
    "        mejor_sig = np.max(Q[nuevo_estado], axis=0)\n",
    "        Q[estado][accion] = (1 - alpha) * Q[estado][accion] + alpha * (recompensa_vec + gamma * mejor_sig)\n",
    "\n",
    "        estado = nuevo_estado\n",
    "        recompensa_acc += recompensa_vec\n",
    "\n",
    "    recompensas_hist.append(recompensa_acc)\n",
    "    frontera_pareto = actualizar_pareto(frontera_pareto, recompensa_acc)\n",
    "\n",
    "    if ep % 50 == 0:\n",
    "        print(f\"Episodio {ep}, Recompensa acumulada: {recompensa_acc}\")\n",
    "\n",
    "entorno.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recompensas_arr = np.array(recompensas_hist)\n",
    "plt.plot(recompensas_arr[:, 0], label='Recompensa 1')\n",
    "plt.plot(recompensas_arr[:, 1], label='Recompensa 2')\n",
    "plt.title(\"Recompensas por episodio\")\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Recompensas\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frontera_pareto = np.array(frontera_pareto)\n",
    "plt.scatter(frontera_pareto[:, 0], frontera_pareto[:, 1], color='red')\n",
    "plt.title(\"Frontera de Pareto aproximada\")\n",
    "plt.xlabel(\"Objetivo 1\")\n",
    "plt.ylabel(\"Objetivo 2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}