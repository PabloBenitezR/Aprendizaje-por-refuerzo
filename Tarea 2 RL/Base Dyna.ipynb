{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implememtaci칩n base del algoritmo Dyna Q\n",
    "\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par치metros\n",
    "alpha = 0.1              # tasa de aprendizaje\n",
    "gamma = 0.995             # factor de descuento\n",
    "epsilon = 0.1            # epsilon\n",
    "n_planning = 5           # planeaci칩n\n",
    "num_episodios = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabla Q\n",
    "q_shape = bins_por_variable + [2] \n",
    "Q = np.zeros(q_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(num_episodios):\n",
    "    obs, _ = env.reset()\n",
    "    estado_disc = discreto(obs)\n",
    "    terminado = False\n",
    "    recompensa_total = 0\n",
    "    \n",
    "    while not terminado:\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            accion = np.random.randint(2)\n",
    "        else:\n",
    "            accion = np.argmax(Q[estado_disc])\n",
    "        \n",
    "        \n",
    "        obs_sig, recompensa, done, truncated, _ = env.step(accion)\n",
    "        terminado = done or truncated\n",
    "        recompensa_total = recompensa_total + recompensa\n",
    "        pasos_totales = pasos_totales + 1\n",
    "        \n",
    "        estado_sig_disc = discreto(obs_sig)\n",
    "        \n",
    "        # actualizar Q \n",
    "        mejor_q_sig = np.max(Q[estado_sig_disc])\n",
    "        td_error = recompensa + gamma * mejor_q_sig - Q[estado_disc + (accion,)]\n",
    "        Q[estado_disc + (accion,)] = Q[estado_disc + (accion,)] + alpha * td_error\n",
    "        Model[(estado_disc, accion)] = (recompensa, estado_sig_disc)\n",
    "        \n",
    "        # planeaci칩n\n",
    "        for _ in range(n_planning):\n",
    "            (s_plan, a_plan) = random.choice(list(Model.keys()))\n",
    "            (r_plan, s_sig_plan) = Model[(s_plan, a_plan)]\n",
    "            \n",
    "            mejor_q_plan = np.max(Q[s_sig_plan])\n",
    "            td_error_plan = r_plan + gamma * mejor_q_plan - Q[s_plan + (a_plan,)]\n",
    "            Q[s_plan + (a_plan,)] = Q[s_plan + (a_plan,)] + alpha * td_error_plan\n",
    "        \n",
    "        estado_disc = estado_sig_disc\n",
    "    \n",
    "    \n",
    "    lista_recompensas.append(recompensa_total)\n",
    "    lista_pasos.append(pasos_totales)\n",
    "    \n",
    "    # (Opcional) Ajustar epsilon de forma decreciente\n",
    "    epsilon = max(0.01, epsilon * 0.99)\n",
    "\n",
    "    if (episodio + 1) % 25 == 0:\n",
    "        print(f\"Episodio {episodio+1}, Recompensa = {recompensa_total}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promedio(datos, vnv=10): # vnv: para el promedio de los episodios\n",
    "    return np.convolve(datos, np.ones(vnv)/vnv, mode='valid')\n",
    "\n",
    "vnv = 10\n",
    "recompensas_soft = promedio(lista_recompensas, vnv)\n",
    "pasos_soft = lista_pasos[vnv-1:]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
