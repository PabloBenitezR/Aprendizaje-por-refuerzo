{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paquetes de la implementación\n",
    "import numpy as np\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "# paquetes de las gráficas\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Creación del entorno y parámetros\n",
    "# -------------------------------\n",
    "env = gym.make('ALE/Breakout-v5')\n",
    "num_acciones = env.action_space.n  # Número de acciones\n",
    "# En Breakout, la observación es una imagen (por ejemplo, 210x160x3)\n",
    "# Usaremos preprocesamiento para reducirla\n",
    "\n",
    "# Parámetros de Dyna-Q\n",
    "alpha = 0.1         # Tasa de aprendizaje\n",
    "gamma = 0.995        # Factor de descuento\n",
    "epsilon = 0.3       # Mayor exploración inicial (puedes ajustar)\n",
    "n_planning = 50    # Pasos de planeación por paso real\n",
    "num_episodios = 500\n",
    "\n",
    "# Variables para llevar registro\n",
    "pasos_totales = 0\n",
    "lista_pasos = []         # Pasos acumulados al final de cada episodio\n",
    "lista_recompensas = []   # Recompensa total por episodio\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Preprocesamiento y discretización del estado\n",
    "# -------------------------------\n",
    "def preprocess_observation(obs):\n",
    "    \"\"\"\n",
    "    Convierte la imagen de Breakout (RGB) a una imagen binaria de 16x16.\n",
    "    \"\"\"\n",
    "    # Convertir a escala de grises (usando coeficientes estándar)\n",
    "    gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "    # Redimensionar a 16x16 (interpolación por área)\n",
    "    small = cv2.resize(gray, (16, 16), interpolation=cv2.INTER_AREA)\n",
    "    # Binarizar: píxeles mayores que 128 se convierten en 1, el resto en 0\n",
    "    binary = (small > 128).astype(np.uint8)\n",
    "    return binary\n",
    "\n",
    "def get_state_key(obs):\n",
    "    \"\"\"\n",
    "    Procesa la observación y la convierte en una tupla, que servirá de clave.\n",
    "    \"\"\"\n",
    "    processed = preprocess_observation(obs)\n",
    "    return tuple(processed.flatten())\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Inicialización de la tabla Q y el modelo\n",
    "# -------------------------------\n",
    "# Tabla Q: diccionario con clave: (estado, acción) y valor: Q(s,a)\n",
    "Q = {}\n",
    "\n",
    "def obtener_Q(estado, accion):\n",
    "    key = (estado, accion)\n",
    "    if key not in Q:\n",
    "        Q[key] = 0.0\n",
    "    return Q[key]\n",
    "\n",
    "def actualizar_Q(estado, accion, td_error):\n",
    "    key = (estado, accion)\n",
    "    if key not in Q:\n",
    "        Q[key] = 0.0\n",
    "    Q[key] += alpha * td_error\n",
    "\n",
    "# Modelo: diccionario que mapea (estado, acción) -> (recompensa, siguiente_estado)\n",
    "Model = {}\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Entrenamiento con Dyna-Q\n",
    "# -------------------------------\n",
    "for episodio in range(num_episodios):\n",
    "    # env.reset() devuelve (obs, info)\n",
    "    obs, info = env.reset()\n",
    "    estado = get_state_key(obs)\n",
    "    terminado = False\n",
    "    recompensa_total = 0\n",
    "\n",
    "    while not terminado:\n",
    "        # Selección de acción con política ε-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            accion = np.random.randint(num_acciones)\n",
    "        else:\n",
    "            q_vals = [obtener_Q(estado, a) for a in range(num_acciones)]\n",
    "            accion = int(np.argmax(q_vals))\n",
    "        \n",
    "        # Ejecutar la acción en el entorno\n",
    "        next_obs, recompensa, terminated, truncated, info = env.step(accion)\n",
    "        terminal = terminated or truncated\n",
    "        pasos_totales += 1\n",
    "        recompensa_total += recompensa\n",
    "\n",
    "        next_estado = get_state_key(next_obs)\n",
    "\n",
    "        # Actualización de Q mediante Q-learning\n",
    "        best_next_q = max([obtener_Q(next_estado, a) for a in range(num_acciones)])\n",
    "        td_error = recompensa + gamma * best_next_q - obtener_Q(estado, accion)\n",
    "        actualizar_Q(estado, accion, td_error)\n",
    "\n",
    "        # Actualizar el modelo con la transición observada\n",
    "        Model[(estado, accion)] = (recompensa, next_estado)\n",
    "\n",
    "        # Planeación: simulamos transiciones del modelo\n",
    "        if len(Model) > 0:\n",
    "            for _ in range(n_planning):\n",
    "                key_sim = random.choice(list(Model.keys()))\n",
    "                r_sim, next_state_sim = Model[key_sim]\n",
    "                a_sim = key_sim[1]\n",
    "                best_next_q_sim = max([obtener_Q(next_state_sim, a) for a in range(num_acciones)])\n",
    "                td_error_sim = r_sim + gamma * best_next_q_sim - obtener_Q(key_sim[0], a_sim)\n",
    "                actualizar_Q(key_sim[0], a_sim, td_error_sim)\n",
    "        \n",
    "        estado = next_estado\n",
    "        terminado = terminal\n",
    "\n",
    "    lista_recompensas.append(recompensa_total)\n",
    "    lista_pasos.append(pasos_totales)\n",
    "    \n",
    "    if (episodio + 1) % 100 == 0:\n",
    "        print(f\"Episodio {episodio+1}, Recompensa = {recompensa_total}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Gráfica: Recompensa Promedio vs. Pasos\n",
    "# -------------------------------\n",
    "def promedio_movil(datos, ventana=10):\n",
    "    return np.convolve(datos, np.ones(ventana)/ventana, mode='valid')\n",
    "\n",
    "ventana = 10\n",
    "recompensas_suavizadas = promedio_movil(lista_recompensas, ventana)\n",
    "pasos_suavizados = lista_pasos[ventana-1:]  # Alinear el promedio móvil\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(pasos_suavizados, recompensas_suavizadas, label=f\"Recompensa Promedio (ventana={ventana})\")\n",
    "plt.xlabel(\"Pasos Acumulados\")\n",
    "plt.ylabel(\"Recompensa Promedio\")\n",
    "plt.title(\"Dyna-Q Tabular en Breakout (ALE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Evaluación de la Política Final\n",
    "# -------------------------------\n",
    "def evaluar_politica(env, Q, n_episodios=10):\n",
    "    recompensas = []\n",
    "    for _ in range(n_episodios):\n",
    "        obs, info = env.reset()\n",
    "        estado = get_state_key(obs)\n",
    "        terminado = False\n",
    "        recompensa_total = 0\n",
    "        while not terminado:\n",
    "            q_vals = [obtener_Q(estado, a) for a in range(num_acciones)]\n",
    "            accion = int(np.argmax(q_vals))\n",
    "            next_obs, recompensa, terminated, truncated, info = env.step(accion)\n",
    "            terminal = terminated or truncated\n",
    "            recompensa_total += recompensa\n",
    "            estado = get_state_key(next_obs)\n",
    "            terminado = terminal\n",
    "        recompensas.append(recompensa_total)\n",
    "    return recompensas\n",
    "\n",
    "env_eval = gym.make('ALE/Breakout-v5')\n",
    "recompensas_eval = evaluar_politica(env_eval, Q, n_episodios=10)\n",
    "env_eval.close()\n",
    "\n",
    "media_recompensa = np.mean(recompensas_eval)\n",
    "std_recompensa = np.std(recompensas_eval)\n",
    "\n",
    "print(\"\\nResultados de la política final en 10 experimentos:\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"| {'Métrica':<12} | {'Valor':<20} |\")\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"| {'Media':<12} | {media_recompensa:<20.2f} |\")\n",
    "print(f\"| {'Desviación':<12} | {std_recompensa:<20.2f} |\")\n",
    "print(\"----------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
